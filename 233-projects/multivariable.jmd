# Functions of multiple variables

This notebook discusses how we can deal with functions $f:R^n
\rightarrow R$, that is which take values in $R^n$ and give back a
real number. An example would be $f(x,y) = x^2 + y^3$. For the most part we specialize to $n=2$.



## Graphs

For functions from $R^2$ to $R$, we can visualize them in various ways. We discuss surface plots and contour plots

### Surface plots

A plot of the set of points $(x ,y, f(x,y))$ is known as a surface plot. These may be graphed with the `PyPlot` package.

<p>

For a numeric function, we need to create values for $x$, $y$ and then
compute a grid of numbers for $f(x,y)$. For example,

```
using PyPlot
pygui(false)

f(x, y) = x^2 + y^2

xs = linspace(-5, 5)
ys = linspace(-5, 5)
zs = [f(x,y) for y in ys, x in xs]

plot_surface(xs, ys, zs)
```


If you are using `SymPy` and `x` and `y` are symbolic, then we can
pass in an expression and a region (`x1,x2,y1,y2`) to plot over:

```
using SymPy
pygui(false)
x, y = symbols("x, y", real=true)
plot_surface(x^2 + y^2, -5, 5, -5, 5)
```

### Contour plots

An alternative plot is the contour plot. This graphs in the
$x-y$ plane the solutions to $f(x,y) = c$ for different values of
$c$. Contour plots are produced in `PyPlot` with the `contour`
function. The same data preparation as `plot_surface` is used:

```
using PyPlot
pygui(false)

f(x, y) = x^2 + y^2
xs = linspace(-5, 5)
ys = linspace(-5, 5)
zs = [f(x,y) for y in ys, x in xs]

contour(xs, ys, zs)
```

The named argument `levels=[a,b,...]` will bypass the automatic generation
of the levels to plot and will instead plot those corresponding to the
vector of values.

<p>

With `PyPlot`, contour plots can also be generated by `contour3D`, which will place
them in a 3D view. The named argument `offset=0` will arrange for the
contours to live on the plane $z=0$, otherwise, they will placed in 3
dimensions.

<p>

Again, within `SymPy`, an expression in two symbolic variables can be
passed in as the first argument:

```
x, y = symbols("x, y", real=true)
contour3D(x^2 - y^2, -5, 5, -5,5)
```

### Combining plots


A contour plot and the surface plot can be rendered in the same graphic. For example,

```
using PyPlot
pygui(false)

f(x, y) = x^2 + y^2

xs = linspace(-5, 5)
ys = linspace(-5, 5)
zs = [f(x,y) for y in ys, x in xs]

plot_surface(xs, ys, zs)
contour3D(xs, ys, zs, offset=0)
```

### Interactive elevation and azimuth adjustment

The viewing elevation and azimuth can be adjusted with `PyPlot`.  (If
we were using
[spherical coordinates](http://en.wikipedia.org/wiki/Spherical\_coordinate_system),
$(\rho, \theta, \phi)$ to describe position, then elevation would be
the $90 - \theta$ and azimuth the $\phi$, in degrees.)  The following
command will do so: `ax=gca(); ax[:view_init](azim=200, elev=22)`
(The `gca` call returns an axes element for which its `view_init`
method is called with the specified values.)

<p>


Specifying the azimuth and elevation can be made interactive using the `Interact` package by the following pattern:

```
f(x, y) = x^2 + y^2
xs = linspace(-5, 5)
ys = linspace(-5, 5)
zs = [f(x,y) for y in ys, x in xs]

using Interact

fig = figure()
@manipulate for azim in -90:90, elev in 0:90; withfig(fig) do

    plot_surface(xs, ys, zs)
    contour3D(xs, ys, zs, offset=0)

    ax = gca()
	ax[:view_init](azim=azim, elev = elev)
  end
end
```

The key is to put the drawing commands within the `@manipulate`
call. For `PyPlot` usage, the `withfig(fig)` part is needed to get
interactivity, though that isn't necessarily the case with other
graphic programs, such as `Gadfly`.


### Adding vectors to a plot

A vector can be added to a plot. With `PyPlot`, in 2 dimensions the
`arrow` function is available, in 3 dimensions we can draw "headless"
vectors with `plot3D`. This function (which is also provided by `SymPy` when `PyPlot` is used) will choose
based on the length of the input. It takes two vectors, a position
vector, `p`, and the vector, `v`, to plot:

```
function add_arrow(p::Vector, v::Vector, args...; kwargs...)
     n = length(p)
     if n == 2
       arrow(p..., v...; kwargs...)
     elseif n==3
       out = [hcat(p,p+v)'[:,i] for i in 1:n]
       plot3D(out..., args...; kwargs...)
     end
end
```

Here we draw the contours of some function `g` with specified levels
and add a few vectors:

```
pygui(false)

g(x,y) = x^2 + 2y^2
xs = ys = linspace(-2,2)
zs = [g(x,y) for y in ys, x in xs]
contour(xs, ys, zs, levels=[1,2,3,4,5])

add_arrow([ 1,1], 0.25*[ 1,-1])  ## start at [1,1] in direction [1,-1]
add_arrow([-1,1], 0.25*[-1, 1])
```




### Parametrically described surfaces

Not all surfaces are easily described by points of the form
$(x,y,f(x,y))$ for some function $f$ in the same way as not all 2
dimensional curves are well described by the graph of a function. In
the case of two-dimensional curves we parameterized them by describing the $x$ and
$y$ position in terms of functions of third variable $t$. In a similar
manner we can parameterize a surface, though it takes two variables to
describe.

<p>


Take for example the task of plotting a sphere. A sphere is most
easily described in
[spherical coordinates](http://en.wikipedia.org/wiki/Spherical\_coordinate_system)
which map $(r, \theta, \phi)$ into $(x,y,z)$ by:

```
## Spherical
X(r,theta,phi) = r * sin(theta) * sin(phi)
Y(r,theta,phi) = r * sin(theta) * cos(phi)
Z(r,theta,phi) = r * cos(theta)
```

(We use capital letters, as we have used `x` and `y` for symbolic
objects previously in this notebook and redefining them as variables
causes an error.)

<p>


We then generate `xs`, `ys` and `zs` by iterating over a region
described in terms of $\theta$ and $\phi$, as $r$ is constant for a
sphere:

```
pygui(false)

thetas = linspace(0, pi, 250)
phis   = linspace(0, pi/2,250)
xs = [X(1, theta, phi) for theta in thetas, phi in phis]
ys = [Y(1, theta, phi) for theta in thetas, phi in phis]
zs = [Z(1, theta, phi) for theta in thetas, phi in phis]

plot_surface(xs, ys, zs)
```

The repeating of similar lines to define `xs`, `ys`, and `zs` can be
avoided, but it isn't much nicer with this solution:

```
H(theta, phi) = [X(1, theta, phi), Y(1, theta, phi), Z(1, theta, phi)]
out = [ [H(theta, phi)[i] for theta in thetas, phi in phis] for i in 1:3]
plot_surface(out...)
```


### Tangent planes,

A plane in 3 dimensions is described by a point and two vectors. The two
vectors must be non-collinear, which means their cross product should
be non-zero. Though a bit tedious, a graph is easy to make once we
parameterize the plane:

```
pygui(false)

p = [1,1,1]
u = [1,0,1]
v = [0,1,1]
@assert norm(u × v) > 0   # \times<tab> for ×
plane(a,b) = p + a*u + b*v

as = linspace(-1,1)
bs = linspace(-1,1)

xs = [plane(a, b)[1] for b in bs, a in as]
ys = [plane(a, b)[2] for b in bs, a in as]
zs = [plane(a, b)[3] for b in bs, a in as]

plot_surface(xs, ys, zs)
add_arrow(p, u, color="red")
add_arrow(p, v, color="yellow")
```


## Partial derivatives

For function $f:R^n \rightarrow R$ the concept of a derivative is
extended to _partial derivatives_ for $n$ variables. The partial
variable of $x$ is defined by holding $y$ constant while a derivative
in $x$ is taken.

<p>


The definition has:

$$
\frac{\partial f}{\partial x}(x,y) = \lim_{h \rightarrow 0} \frac{f(x + h,y) - f(x,y)}{h}.
$$

The gradient of $f$, $\nabla f$, is the vector-valued function of
partial derivatives
$[\partial f / \partial x, \partial f / \partial y]$ (taking $R^2$
into $R^2$.)

<p>


This definition lends itself to the numeric approximation using a
forward difference equation via something like the following where a
limit is replaced by a small value of $h$

```
f(x,y) = x^2 - 2x*y + y^3
partialx(f::Function, h=1e-6) = (x,y) -> (f(x+h, y) - f(x, y))/h
partialy(f::Function, h=1e-6) = (x,y) -> (f(x, y+h) - f(x, y))/h
gradient_f(f::Function) = (x,y) -> [partialx(f)(x,y), partialy(f)(x,y)]
```

Finding the partial derivatives numerically could be generalized, but we
don't pursue that. The `Calculus.jl` package provides that feature.


### Symbolic partial derivatives

We can also find partial derivatives using `SymPy`'s `diff`
function. That second argument to that function is to specify a
variable, the third the degree of the derivative (it has a default of
1, so can usually be omitted) . The basic partial derivatives
$\partial f/ \partial x$ and $\partial f/ \partial y$ are found as
follows:

```
xs = u, v = symbols("x, y", real=true)
f(x,y)  = x^2 - 2x*y + y^3
ex = f(x, y)
partial_x = diff(ex, x, 1)
partial_y = diff(ex, y)    # 1 is default degree
[partial_x, partial_y]     # The gradient, or
Sym[diff(ex, s) for s in xs]  # finds gradient in one swoop
```

(We give the list comprehension a little boost to infer that the
output are symbols by using `Sym[...]` instead of just `[...]`.)

<p>


The Hessian matrix is a matrix of second partial derivatives. We can find it as follows:

```
xs = x, y = symbols("x, y", real=true)
f(x,y)  = x^2 - 2x*y + y^3
ex = f(x, y)

hess = Sym[diff(diff(ex, a), b) for b in xs, a in xs]
```


## Interpreting partial derivatives and the gradient

The partial derivative in $x$ at a point, $(a,b)$, can be viewed by
slicing the graph of $f(x,y)$ with the plane $y=b$. This intersection
will result in a function whose derivative is just the partial
derivative.

<p>


The derivative indicates the slope of the tangent line, so the partial
derivative in $x$ at $(a,b)$ indicates the slope of the surface were
one constrained to move in the fixed direction $y=b$.

<p>


The **directional derivative**, $\nabla_v(f)$, is the slope were one
constrained to move in a direction specified by a vector $v$. This
generalizes the partial in $x$ where $v$ may be taken to be $c \cdot
[1,0]$ and the partial in $y$ where $v$ may be taken to be $c \cdot
[0,1]$, for some scalar $c$. For an arbitrary $v$, the partial
derivative is defined by a limit of $(f(x + hv) - f(x))/h$ where $x$
and $v$ are vectors, but may be more easily written in terms of the
gradient via

$$
\nabla_v f(x) = \nabla(f) \cdot v
$$

The above includes the magnitude of $v$ in its interpretation, where
this is the slope of a curve being traversed in the direction of $v$
with speed given by $\| v\|$. It is not uncommon to have a definition
in terms of moving at unit speed, as this is more intrinsic to the
function $f$.

### Gradient is direction of greatest ascent

Take $v$ to be a unit vector, then the magnitude of the the directional derivative is

$$
\| \nabla_v(f)(x) \| = \| \nabla(f) \| \sin(\theta)
$$

where $\theta$ is the angle between the gradient and $v$. Taking $v$
in the direction of the gradient will maximize this magnitude. Hence
the gradient is in the direction of greatest increase of ascent on the
surface.

<p>

That is, if one is on a hill described by $(x,y,f(x,y))$, then walking
so that your $(x,y)$ is in the direction of the gradient will walk in
the steepest possible way. Contrast this to walking along a contour,
$f(x,y) = k$, which by definition will have no slope up or down, as
the $z$ coordinate stays at $k$. In some sense the gradient and the
contour are orthogonal concepts. In fact, if we plot, we can see that
the gradient is perpendicular to the tangent line of the contour. For
example, taking the simple function $f(x,y) = x^2 + y^2$:

```
pygui(false)

f(x,y) = x^2 + y^2
grad_f(x,y) = [2*x, 2*y]
xs = ys = linspace(-3, 3)
zs = [f(x,y) for y in ys, x in xs]
contour(xs, ys, zs, levels = [1,2,4,9])

## add gradient
add_arrow([sqrt(3)/2,1/2], grad_f(sqrt(3)/2,1/2))

## tangent to contour by implicit differentiation solves 2x + 2y y' = 0
## or y' = -x/y (which is in direction [y, -x])
add_arrow([sqrt(3)/2,1/2], [1/2, -sqrt(3)/2])

## make aspect ratio "equal" as follows:
ax = gca()
ax[:axis]("equal")
```

Were this not the case, then the directional derivative in a direction
of the tangent to the contour would have non-zero magnitude (as the
directional derivative is a dot product with the gradient which is
non-zero when the two vectors are and the two vectors are not
perpendicular). That would mean moving in that direction will have a
slope up or down, but that direction is along a contour which is by
definition flat.


### Implicit differentiation with SymPy

The expression above to find the implicit derivative $dy/dx$ for
$f(x,y) = x^2 + y^2$ can be done in `SymPy`, but requires some
work. The key is to define a symbolic function, `F = SymFunction(:F)`
and use that to substitute in for `y`:

```
ex = x^2 + y^2
F = SymFunction(:F)
tmp = diff(subs(ex, y, F(x)), x)  # x^2 + F(x)^2
ex1 = solve(tmp, diff(F(x),x))    # solve for dF/dx in 2x + 2F(x) dF/dx, gives [-x/F(x)]
tl = subs(ex1, F(x), y)           # now it is [-x/y]

Sym[diff(ex,s) for s in [x,y]] ⋅ [1, tl]      # 0 as orthogonal.
```

(This uses the fact that the vector `[1,m]` is parallel to a line with slope `m`.)

### The chain rule, [G(r(x))' =  ∇G(r(x)) ⋅ r'(x)

Of course, the last part is of a more general nature. For example, suppose $r(t) = \langle x(t), y(t) \rangle$ parameterizes the contour $G(x,y) = k$. The $(G \circ r)(t) = k$, a constant so would have derivative $0$. But by the chain rule, $0 = \nabla G(x(t), y(t)) \cdot r'(t)$, so the gradient is always perpendicular to the tangent vector, as was the case above.

## Applications

The fact that the gradient points in the direction of greatest
increase can be exploited. The first generalizes the characterization
of local extrema for differentiable functions as only occurring when
the derivative is zero (a critical point). The same is true for
two-dimensional functions, where a critical point would be where the
gradient is $0$ length. (By assuming differentiable we avoid talking
about those critical points where the derivative is undefined, such as
at a cusp.)

### Optimization

The definition of a _local maximum_ of a function $f$ is not much
different when $f:R^n \rightarrow R$ for $n = 2$ than the $n=1$ case:
$f(a,b)$ is a local maximum of $f$ if $f(a,b) \geq f(x,y)$ for any
$(x,y)$ in an open disk about $(a,b)$. That is, the value $f(a,b)$ is
the largest in some nearby neighborhood, at least. Using a mountain
analogy, this can be the ultimate peak, or a lower peak, it just needs
to be the tallest value in some (open) region. A _local minimum_ has a
similar defintion.

<p>

If $f$ is differentiable and $(a,b)$ is an interior point of the
domain, then one has at a local extrema that *both* partial
derivatives will be 0. This is a direct analogue of the $n=1$ case.

<p>

However, as with the $n=1$ case, it is not enough to know that both
partials are $0$ to conclude you are at a local extrema. An example
where this is not the case is a saddle point. The function $f(x,y) =
y^2 - y^4 - x^2$ demonstrates this at $0$.

```
using PyPlot, Interact
pygui(false)

f1(x,y) = y^2 - y^4 - x^2
x,y = symbols("x,y", real=true)

fig = figure()
@manipulate for azim in -90:90, elev in 0:90; withfig(fig) do
    plot_surface(f1(x,y), -1,1,-1,1)

    ax = gca()
	ax[:view_init](azim=azim, elev = elev)
  end
end
```

There is a test though that involves the Hessian, whose determinant is $d = f_{xx}f_{yy} - f_{xy}^2$:

* $f$ has a local maximum if $f_{xx} < 0$ and $d > 0$
* $f$ has a local minimum if $f_{xx} > 0$ and $d > 0$
* $f$ has a saddle point if $d < 0$
* the test is otherwise inconclusive.

(The second derivative test when $n=1$ is similar to the first, second
and fourth statements without consideration of $d$.)

Let's see this in use: maximize $g(x,y) = 2 + 2x + 2y - x^2 - y^2$
over the triangular region bounded by $x=0$, $y=0$ and $y=9-x$.

```
g(x,y) =  2 + 2x + 2y - x^2 - y^2
h(x,y) = (x>0) & (y>0) & (y < 9-x) ? g(x,y) : NaN

xs = linspace(0,9, 250)
ys = linspace(0, 9, 250)
zs = [h(x,y) for y in ys, x in xs]

plot_surface(xs, ys, zs)
contour3D(xs, ys, zs, offset=0)
```

We can see that the maximum occurs near the origin, and not out at the
edge. Let's find it symbolically:

```
xs = x, y = symbols("x,y", real=true)
grad = Sym[diff(g(x,y),s) for s in xs]
solve(grad)
```

The answer of $y=1$ gives $x=1$ is in the region and has a zero
gradient, as we can check by substituting in these values for $x$ and
$y$ in the gradient expression:

```
subs(grad, x, 1) |> subs(y, 1)
```

The Hessian is found with this pattern:

```
hess = [diff(diff(g(x,y), s1), s2) for s1 in xs, s2 in xs]
```

The value of $g_{xx}$ is the upper left value which is $-2$. By the
theorem, the value $(1,1)$ will correspond to a local maximum if the
determinant is positive, and it is:

```
det(hess)
```

The relative maximum is the value:

```
g(1,1)
```



Is this the maximum value over the region? For that, we need to check
the boundary of the region, that is when $x=0$ or $y=0$ or
$y=9-x$. This is (tediously) done in three steps:

* $x=0$. Then we have the function $h(y) = g(0,y)$ over $[0,9]$. This
  function of a single variable and it has a larger value would occur
  at an endpoint or a critical point:

```
h(y) = g(0,y)
out = solve(diff(h(y), y))
append!(out, [0, 9])
map(h, out)
```

Nope, nothing bigger than $4$.

* $y=0$. Then we have $h(x) = g(x,0)$ over $[0,9]$. Repeating gives
  the same answer:

```
h(x) = g(x,0)
out = solve(diff(h(x), x))
append!(out, [0, 9])
map(h, out)
```

(You could skip this last one by noting the $x$ and $y$ enter symmetrically.

* $y = 9-x$. Here we parameterize by $t$ going from $0$ to $9$:

```
h(t) = g(t, 9-t)
t = Sym("t")
out = solve(diff(h(t), t))
append!(out, [0, 9])
map(h, out)
```

Again, all these values are less than $4$, so we can conclude that $4$
is a local minimum and the absolute minimum over the region.

### Lagrange multipliers

Unconstrained optimization is related to the gradient. A related, but
different question is constrained optimization. A generic description
would be along the lines of

> Maximize $f(x,y)$ given $g(x,y) = k$.

For this problem the gradient of $f$ may never be $0$ along the
contour of $g$. However, a maximal value will be determined by the
gradient. For example, the gradient of $f$ should be perpendicular to
the contour lines of $g$, as otherwise we could move in the direction
of the projection of the gradient of $f$ onto tangent of the contour
line of $g$ and increase our function. However, the gradient of $g$ is
also perpendicular to the tangent of the contour lines of $g$, so in
fact we need the gradient of $f$ and the gradient of $g$ to be
parallel. That is, in addition to satisfying the constraing, a
necessary condition for an optimal value is for some $\lambda$ that

$$
\nabla f(x,y) = \lambda \nabla g(x,y).
$$



This is the basis for the technique of
[Lagrange Multipliers](http://en.wikipedia.org/wiki/Lagrange\_multiplier).


<p>


The following graphic illustrates the method for the problem where we
maximize $f(x,y) = 5 + x + y^2$ over the constraint formed by $g(x,y)
= x^2 + y^2 = 1$. We make a contour plot, then draw for a few points
the gradient of $f$ and the gradient of $g$. When the arrows are
parallel, we have a possible maxima. In the figure this occurs at
$\pi/3$.

```
pygui(false)

g(x,y) = x^2 + y^2
∇g(x,y) = [2x, 2y, 0]

f(x,y) = 5 + x + y^2
∇f(x,y) = [1, 2*y,0] 

uvec(u) = u/norm(u)

xs = linspace(-3,3) ## should be square!
ys = linspace(-3,3)
ts = [0, pi/6, pi/3, pi/2]
zcs = [g(x,y) for y in ys, x in xs]

p = contour3D(xs, ys, zcs,  offset=0, levels=[1,4,9])

## parameterize g(x,y) == 1
rad(t) = [cos(t),sin(t)]

for t in ts
    add_arrow([rad(t)...,0], uvec(∇g(rad(t)...)), color="blue")
    add_arrow([rad(t)...,0], uvec(∇f(rad(t)...)), color="red" )
end
```

This graphic furthers the above by also adding in the surface plot and
adds controls to adjust the elevation and azimuth.


```
f(x,y) = 5 + y^2+ x
g(x,y) = x^2 + y^2   ## constraint

## parameterize g==1
rad(t) = [cos(t), sin(t)]
rho(t) = [rad(t)..., f(rad(t)...)]

xs = linspace(-3,3)
ys = linspace(-3,3)
ts = linspace(0, 2pi)

zs  = [f(x,y) for y in ys, x in xs]
zcs = [g(x,y) for y in ys, x in xs]

fig = figure()
@manipulate for azim in -90:90, elev in 0:90; withfig(fig) do

        plot_surface(xs, ys, zs)
        plot3D([[rho(t)[i] for t in ts] for i in 1:3]..., color="green")
        contour3D(xs, ys, zcs,  offset=0, levels=[1])

        ax = gca()
	    ax[:view_init](azim=azim, elev = elev)
    end
end
```

Rotating the azimuth to 60 shows the peak of the space curve generated
by moving along the surface plot along the contour $g(x,y) =1$
Similarly, one is at -60 degrees.



To solve this problem algebraically, we can use `SymPy` as follows:

```
using SymPy
xs = x, y = symbols("x,y", real=true)

f(x,y) = 5 + x + y^2
g(x,y) = x^2 + y^2
gradf = Sym[diff(f(x,y),s) for s in xs]  # or ∇f which is entered with \nabla<tab>f
gradg = Sym[diff(g(x,y),s) for s in xs]
[gradf gradg]   # gradf in first column, gradg in second
```


We see from the first row that `1 = \lambda 2x` and from the second
`2y= \lambda 2y`. If $y=0$, then $1 = \lambda 2 x$ can have $x$ take
any value. Similarly, If $x=1/2$ then $\lambda=1$ and from $2y=1\cdot
2y$, we see that $y$ can take any value. However, the constraint
imposed by $g(x,y)=1$ limits the values in each case giving us 4
possible answers: $(1,0)$, $(-1,0)$, $(1/2, \sqrt{3}/2)$ and $(1/2,
-\sqrt{3}/2)$. Of these, the latter two can be seen to be associated
with the maximum values along the constraint, the second is the
minimum and the first a local minimum.


<p>


We could use `SymPy`'s `solve` feature to do this directly. First we
introduce a `lambda` variable and then using the multi-variable form
of `solve`:

```
lambda = symbols("lambda", real=true)
out = solve(gradf - lambda*gradg)
```

We see that the `x=1/2` case is found, but we miss the $y=0$ case. To
use the constraint to find $y$, we have:

```
ex = subs(g(x,y), x, 1//2) # or out[1]["x"] to fish out the 1/2
solve(ex - 1, y)
```

The value of $(1/2, \sqrt{3}/2)$ corresponds to an azimuth of 60
degrees, as expected, and can be checked to be a maximum, as can the
value $(1/2, -\sqrt{3}/2)$.


<p>


It can be simpler to solve this by adding the constraint as an
equation. We do this here by pushing it onto the equations to solve:

```
exs = gradf - lambda * gradg
push!(exs, g(x,y) - 1)
out = solve(exs)
```

The display of the values is not very good, but the output gives four answers which we can check via:

```
[f(out[i]["x"], out[i]["y"]) for i in 1:4]
```

The second and third (with `x=1/2`) are the largest, the one with
`x=-1` the smallest, and the one with `x=1` a local extrema, but not
an extrema over the entire set of values specified by the constraint.

<p>

The
[Langragian](https://www.cs.iastate.edu/~cs577/handouts/lagrange-multiplier.pdf)
associated with the constrained problem is $l(x,y,\lambda) = f(x,y) -
\lambda h(x,y)$, where $h(x,y) = g(x,y) - k$. The solutions to $\nabla
l = 0$ are the same as above.

### Some more examples


> Find the maximal volume of box with fixed surface area of 64 square units (problem is from [Paul's Online Math Notes](http://tutorial.math.lamar.edu/Classes/CalcIII/LagrangeMultipliers.aspx) ).

```
xs = a,b,c = symbols("a,b,c", real=true)
Vol(a,b,c) = a*b*c
SA(a,b,d) = 2*(a*b + a*c + b*c)
gradf = Sym[diff(Vol(a,b,c), s) for s in xs]
gradg = Sym[diff(SA(a,b,c), s) for s in xs]
[gradf  gradg]   # gradf in first column,...
```

What is $\lambda$? Here we add the constraint to the equations and let `SymPy` do the work.

```
lambda = symbols("lambda", real=true)
exs = gradf - lambda * gradg
push!(exs, SA(a,b,c) - 64)
out = solve(exs, [lambda, xs...])
out[2]
```

The two answers differ only by minus signs. Only the second is
possible and yields a cube with sides of length $4\sqrt{6}/3$.




> Find the minimal surface area of a box with fixed volume 64 cubic units (problem is from [Paul's Online Math Notes](http://tutorial.math.lamar.edu/Classes/CalcIII/LagrangeMultipliers.aspx) ).

The basic equations are identical, save for the constraint being
switched. We have then:

```
xs = a,b,c = symbols("a,b,c", real=true)
lambda = symbols("lambda", real=true)

Vol(a,b,c) = a*b*c
SA(a,b,d) = 2*(a*b + a*c + b*c)
gradf = Sym[diff(Vol(a,b,c), s) for s in xs]
gradg = Sym[diff(SA(a,b,c), s) for s in xs]

exs = gradf - lambda * gradg
push!(exs, Vol(a,b,c) - 64)
solve(exs)
```

This too has a solution which is a cube, here with side lengths
of 4. However, note we minimized here, but maximized in the previous
question.




> A right circular can is to hold 355 $mm^3$. Find the dimensions of the radius, $r$, and height, $h$, that minimize surface area. (Problem modified from one [here](http://math.etsu.edu/multicalc/prealpha/Chap2/Chap2-9/printversion.pdf).)


```
xs = radius, height  = symbols("radius,height", real=true)

Vol(r,h) = PI*r^2* h
SA(r,h) = 2*PI*r^2 + 2*PI*r*h
gradf = Sym[diff(SA(radius, height), s) for s in xs]
gradg = Sym[diff(Vol(radius, height), s) for s in xs]


lambda = symbols("lambda", real=true)
exs = gradf - lambda * gradg
push!(exs, Vol(radius,height) - 355)
solve(exs )[1]
```

(There is only one answer, so we use `[1]` to display just that, as otherwise the display is not great.)

If we had solved without adding the extra constraint, we see the relationship between `h` and `r` more clearly:

```
solve(gradf - lambda * gradg)[2]
```

That `height=2 radius` says the profile of this can is a square with height equal to the diameter.

