# Questions to be handed in on Newton's Method:


Begin by loading our package for plotting and the Roots package

```
using Gadfly			
using Roots
```
----

### Quick background

Read about this material here: [Newton's Method](http://mth229.github.io/newton.html).

For the impatient, symbolic math -- as is done behind the scenes at
the Wolfram alpha web site -- is pretty nice. For so many problems it
can easily do what is tedious work. However, for some questions, only
numeric solutions are possible. For example, there is no general
formula to solve a fifth order polynomial, the way there is a
quadratic formula. Even an innocuous polynomial like $f(x) = x^5 - x -
1$ has no easy algebraic solution:

```
import SymPy			# a symbolic math program
x = Sym("x")
solve(x^5 - x - 1)
```

We see that `SymPy` basically punts on this question.

Numeric solutions are available. As this is a polynomial, we could use the `roots` function for `Roots`:

```
f(x) = x^5 - x - 1
roots(f)
```

We see 5 roots -- as expected from a fifth degree polynomial -- with
one real root (the one with `0.0im`) that is approximately
1.1673. Finding such a value usually requires some iterative
root-finding algorithm (though not in the case above which uses linear
algebra). For polynomials, the `fzeros` function uses such an
algorithm _for polynomials_ to find the real roots:

```
fzeros(f)                # no a, b range needed for polynomials.
```

Newton's method is a root-finding algorithm.  Like the bisection
method discussed earlier, it is an _iterative algorithm_. The
algorithm starts with some _guess_ for a _zero_ to an equation $f(x) =
0$. If this guess is called $x_0$, then the algorithm gives a *new
(and improved)* guess $x_1$. It is expected that $x_1$ is a better
guess, but may not be the best that can be. The algorithm is then
repeated _again_ to produce $x_2$. This is done until some guess $x_n$
is as close as we can get or the algorithm fails for some reason. The
_approximate zero_ is taken to be $x_n$.

What is the algorithm for Newton's method? It is simple. If we start with some $x_i$, then
$x_{i+1}$ is given by the intersection point of the $x$-axis of the
tangent line of $f(x)$ at $x_i$. Mathematically
then we can equate our two methods to compute the slope of a tangent
line:

$$
f'(x_i) = \frac{f(x_i) - 0}{x_i - x_{i+1}}
$$

Or, solving for $x_{i+1}$:

$$
x_{i+1} = x_i - f(x_i)/f'(x_i)
$$

Let's see this algorithm for `f(x) = x^3−2x−5`, a function that Newton
himself considered. He was looking for a solution near $2$, so let's
start there:

```
x = 2
f(x) = x^3 - 2x -5
fp(x) = 3x^2 - 2		# done by hand
```

We don't need to track the index ($x_0$, $x_1$, ...) as when we write
the following expression, the next value is just assigned to `x` using
the _current_ value of `x` when computed:

```
x = x - f(x) / fp(x)
x, f(x)				# display both the new guess, x,  and the value f(x)
```

The value of $2.1$ is a better guess, but not near the actual
answer. We simply repeat to (hopefully) get a better guess:

```
x = x - f(x) / fp(x)
x, f(x)
```

Here are a few more repeats:


```
x = x - f(x) / fp(x)
x, f(x)
```


```
x = x - f(x) / fp(x)
x, f(x)
```

The value of `f(x)` is now *basically* 0, and any further updates to
`x` do not change its value. We see that the algorithm has converged
to an answer, `x`, and the fact that it is a zero is confirmed by the
value of `f(x)`.



Repeating steps in `IJulia` can be a bit of a chore. There a few 
ways to make this easier. For example, encapsulate the algorithm into a function or
use a programming construct to repeat the task.

For the former, you might have:

```
newt(x, f, fp) = x - f(x)/fp(x)
```

and then for a given f, do something like

```
f(x) =  x^3 - 2x -5; fp(x) = 3x^2 - 2
newt(x) = newt(x, f, fp)
x = 2.0
newt(newt(newt(newt(newt(x)))))
```

That is kinda ugly. Here we use a programming construct, a *macro*, to
repeat some _expression_ 5 times. (This macro basically replaces the
expression internally with 5 repeats of the expression.)

```
macro take5(body) quote Float64[$(esc(body)) for _ in 1:5] end end # take5 macro
```

Macros are prefaced with a `@` in their name and are typically called
without parentheses:

```
x = 2				# starting value
@take5     x = x - f(x) / fp(x)
```

and to see that `x` has been updated we have:

```
x, f(x)
```




 
### Questions


* Apply Newton's Method to the function $f(x) = \sin(x)$ with an
  initial guess $3$. (This was historically used to compute many
  digits of $\pi$ efficiently.) What is the answer after 5 iterations?
  What is the value of `sin` at the answer?


The value of $f(x)$ after 5 iterations is:

```
```

```julia
using Roots
macro take5(body) quote [$(esc(body)) for _ in 1:5] end end # take5 macro
f(x) = sin(x); fp(x) = cos(x)
x = 3
@take5 x = x - f(x)/fp(x)
val = x
numericq(val, 1e-8, "5 iterations of Newton's method for sin(x), x0=3")
```


The value of $f(x)=\sin(x)$ at this approximate zero:

```
```

```julia
numericq(f(val), 1e-4, "value of f at approximate zero")
```

* Use Newton's method to find a zero for the function
  $f(x)=x^5-x-1$. Start at $x=1.6$. What is the approximate root after
  5 iterations? What is the value of $f(x)$ for your answer? If you do
  one or two more iterations, will your guess be better?

The value after 5 iterations

```
```

```julia
f(x) = x^5 - x - 1; fp(x) = 5x^4 - 1
x = 1.6
@take5 x = x - f(x)/fp(x)
val = x
numericq(val, 1e-4, "5 iterations of Newton's method for \\( x^5 -x - 1 \\), x0=1.6")
```

The value of $f(x)$:

```
```

```julia
numericq(f(val), 1e-8, "value of f(x)")
```
The value after two more iterations:

```
```

```julia
x = x - f(x)/fp(x)
x = x - f(x)/fp(x)
val = x
numericq(val, 1e-8, "7 iterations of Newton's method for \\( x^5 -x - 1 \\), x0=1.6")
```


* Use Newton's method to find a zero of the function $f(x) = \cos(x) -
  x$. Make a graph to identify an initial guess.

Show your commands below

```
```

```julia
longq("Commands to find the zero of \\( \\cos(x) -x \\)")
```

The value of the approximate zero:

```
```


```julia
f(x) = cos(x) - x
val = fzero(f, 1)
numericq(val, 1e-3, "zero of cos(x) - x")
```

### Using `D` for the derivative

If the function `f(x)` allows it, the `D` operator from the `Roots`
package can simplify the Newton's method algorithm, as the derivative
need not be computed by hand. In this case, the algorithm in `julia`
becomes `x = x - f(x)/D(f)(x)`.

* Use Newton's method to find an intersection point of $f(x) =
  e^{-x^2}$ and $g(x)=x$. (Look at $h(x) = f(x) - g(x) = 0$.) Start
  with a guess of $0$.


```
```

```julia
h(x) = exp(-x^2) - x
val = newton(h, 0)
numericq(val, 1e-10, "x-value of intersection point")
```

* Use Newton's method to find *both* positive intersection points of
  $f(x) = e^x$ and $g(x) = 2x^2$. Make a graph to identify good
  initial guesses.

The smallest value is:

```
```

```julia
h(x) = exp(x) - 2x^2
vals = [fzero(h, x0) for x0 in [1.5, 2.5]]
numericq(vals[1], 1e-5, "smaller zero")
```

The largest value is:

```
```

```julia
numericq(vals[2], 1e-5, "larger zero")
```


### Using `newton` and `fzero` from the `Roots` package

The `newton` function in the `Roots` package will compute newton's method. For example:

```
f(x) = sin(x)
fp(x) = cos(x)
x = 3
newton(f, fp, x)
```

The extra argument `verbose=true` will show the iterations:

```
newton(f, fp, 3, verbose=true)
```


However, the `fzero` function -- that we have seen before -- will use
a derivative-free algorithm, similar to Newton's method to find a
zero. So, the above zero can also be found with:

```
fzero(sin, 3)
```

(That is right, `fzero` can be used two different ways -- at
least. Above it is called with an initial guess. Previously, we called
it with a bracketing interval, as in `fzero(sin, [3,4])`. If you
specify a bracketing interval, `fzero` will use an algorithm
guaranteed to converge. If you just specify an initial guess, the
convergence is generally faster, but may not happen.)

----


* find a zero of $f(x) = x\cdot (2+\ln(x))$ starting at $1$. What is
  your answer? How small is the function for this value?

What is the value of the zero?

```
```

```julia
f(x) = x * ( 2 + log(x))
val = fzero(f, 1)
numericq(val, 1e-6, "zero of x * ( 2 + log(x)) starting at 1")
```

The value of the function at the zero?

```
```

```julia
numericq(f(val), 1e-8, "f(xstar)")
```

* Use `fzero` to find all zeros of the function $f(x) = 2\sin(x) -
  \cos(2x)$ in $[0, 2\pi]$. (Graph first to see approximate answers.)

```
```

```julia
longq("Zeros of 2sin(x) - cos(2x) on [0,2pi]", "\\verb+[fzero(f, x) for x in [0.5, 3]]+ gives .375, 2.767")
```


* Use `fzero` to find when the derivative of $f(x) = 5/\cos(x) +
  7/\sin(x)$ is $0$ in the interval $(0, \pi/2)$.

```
```

```julia
f(x) = 5/cos(x) + 7/sin(x)
val = fzero(D(f), pi/4)
numericq(val, 1e-5, "zero of derivative of \\( 5/cos(x) + 7/sin(x) \\)")
```




### When Newton's method fails

Newton's method can fail due to various cases:

1) the initial guess is not close to the zero

2) the derivative, $|f'(x)|$ is too small

3) the second derivative $|f''(x)|$ is too big, or possibly undefined

* Earlier was considered the roots of $f(x) = x^5 - x - 1$. Try
  Newton's method with an initial guess of $x_0=0$ to find a real
  root. Why does this fail? (You can look graphically. Otherwise, you
  could look at the output of `newton` with this extra argument:
  `newton(f, fp, x0, verbose=true)`.

```
```

```julia
longq("Why does Newton's method fail?", "Too far from x0 (on other side of min)")
```

* Let `f(x) = abs(x)^(1/3)`. Starting at `x=1`, Newton's method will
  fail to converge. What happens? Are any of the above 3 reason's to
  blame?

```
```

```julia
longq("Why does Newton's method fail?", "f'' doesn't exist, basically too big")
```

### Quadratic convergence

When Newton's method converges to a *simple zero* it is said to have
*quadratic convergence*. A simple zero is one with multiplicity 1 and
quadratic convergence says basically that the error at the $i+1$st
step is like the error for $i$th step squared. In particular, if the
error is like $10^{-3}$ on one step, it will be like $10^{-6}$, then
$10^{-12}$ then $10^{-24}$ on subsequent steps. (Which is typically
beyond the limit of a floating point approximation.) This is why one
can *usually* take just 5 steps to get to an answer.

Not so for multiple roots. 

* For the function `f(x) = (8x*exp(-x^2) -2x - 3)^8`, starting with
  `x=-2.0` Newton's method will converge, but it will take many steps
  to get to an answer that has $f(x)$ around $10^{-16}$. How many?
  Roughly how many iterations do you need? (A single call of 
  `@take5 x = x-f(x)/D(f)(x)` gives an answer with `f(x) = 0.00028` only.)

```
```

```julia
choices = ["about $k steps" for k in [5,10,15,20,25,30, 35, 40]]
ans = 6
radioq(choices, ans, "How many steps to get convergence of newton's method?")
```


* Repeat the above with `f(x) = 8x*exp(-x^2) -2x - 3` -- there is no
  extra power of $8$ here -- and again, starting with
  `x=-2.0`. Roughly how many iterations are needed now?

```
```

```julia
choices = ["about $k steps" for k in [5,10,15,20,25,30, 35, 40]]
ans = 1
radioq(choices, ans, "How many steps to get convergence of newton's method?")
```
